import requests
from bs4 import BeautifulSoup
import json
import os
import threading
from pymongo import MongoClient
from datetime import datetime
from flask import Flask, jsonify
import offreBot  # Fichier contenant MONGO_URI et autres constantes

# Cr√©er l'application Flask
app = Flask(__name__)

class JobScraper:
    """Scraper pour r√©cup√©rer les offres d'emploi et les stocker dans MongoDB."""

    def __init__(self, url, mongo_uri, db_name, collection_name):
        """Initialisation du scraper avec l'URL cible et la connexion MongoDB."""
        self.url = url
        self.headers = {"User-Agent": "Mozilla/5.0"}
        self.client = MongoClient(mongo_uri, tls=True, tlsAllowInvalidCertificates=True)
        self.db = self.client[db_name]
        self.collection = self.db[collection_name]

        # V√©rification de la connexion MongoDB
        try:
            self.client.server_info()
            print("‚úÖ Connexion r√©ussie √† MongoDB")
        except Exception as e:
            print(f"‚ùå Erreur de connexion √† MongoDB : {e}")
            exit(1)

    @staticmethod
    def categorize_job(title):
        """Attribue une cat√©gorie √† une offre en fonction de son titre."""
        title_lower = title.lower()
        categories = {
            "Informatique / IT": ["d√©veloppeur", "it", "digital", "logiciel", "technicien"],
            "Finance / Comptabilit√©": ["finance", "comptable", "audit", "gestion des risques"],
            "Communication / Marketing": ["communication", "marketing", "publicit√©"],
            "Conseil / Strat√©gie": ["consultant", "analyse", "conseil", "business"],
            "Transport / Logistique": ["transport", "logistique", "mobilit√©"],
            "Ing√©nierie / BTP": ["ing√©nieur", "technicien", "construction", "chantier"],
            "Sant√© / M√©dical": ["sant√©", "h√¥pital", "m√©decin", "infirmier", "pharmacie"]
        }

        for category, keywords in categories.items():
            if any(word in title_lower for word in keywords):
                return category
        return "Autre"

    def fetch_html(self):
        """R√©cup√®re le HTML de la page web."""
        try:
            response = requests.get(self.url, headers=self.headers)
            response.raise_for_status()
            return response.text
        except requests.RequestException as e:
            print(f"‚ùå Erreur lors de la r√©cup√©ration de la page : {e}")
            return None

    def extract_jobs_from_html(self, html):
        """Extrait les offres d'emploi avec BeautifulSoup."""
        soup = BeautifulSoup(html, "html.parser")
        jobs = []

        for cols3_div in soup.find_all("div", class_="Cols3"):
            for job_card in cols3_div.find_all("div", class_="Cols3_item"):
                title_element = job_card.find("p")
                company_location = job_card.find("a").text.strip().split("\n")

                title = title_element.text.strip() if title_element else "N/A"
                company = company_location[0].strip() if len(company_location) > 0 else "N/A"
                location = company_location[1].strip() if len(company_location) > 1 else "N/A"
                link_element = job_card.find("a")

                link = "https://www.mediacongo.net/" + link_element["href"] if link_element else "N/A"

                jobs.append({
                    "title": title,
                    "company": company,
                    "location": location,
                    "url": link
                })
        return jobs

    def extract_full_text(self, url):
        """R√©cup√®re et nettoie tout le texte d'une offre d'emploi."""
        try:
            response = requests.get(url, headers=self.headers)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, "html.parser")

            # Supprime les balises inutiles
            for tag in soup(["script", "style", "noscript", "iframe", "meta", "header", "footer"]):
                tag.extract()

            return soup.get_text(separator="\n", strip=True)
        except requests.RequestException as e:
            print(f"‚ùå Erreur lors de l'extraction du texte : {e}")
            return None

    def summarize_with_mistral(self, text):
        """Appelle l'API de Mistral pour r√©sumer l'offre d'emploi."""
        try:
            response = requests.post(
                url="https://openrouter.ai/api/v1/chat/completions",
                headers={
                    "Authorization": f"Bearer {offreBot.MISTRAL_API_KEY}",
                    "Content-Type": "application/json",
                },
                data=json.dumps({
                    "model": "mistralai/mistral-small-24b-instruct-2501:free",
                    "messages": [{"role": "user", "content": f'{offreBot.SCRIPT} \n {text}'}]
                })
            )

            response_data = response.json()

            if "choices" not in response_data:
                print("‚ùå Erreur: L'API Mistral ne contient pas 'choices'.")
                return None

            return response_data['choices'][0]['message']['content']
        except requests.exceptions.JSONDecodeError:
            print("‚ùå Erreur: L'API Mistral a renvoy√© un JSON invalide.")
            return None
        except Exception as e:
            print(f"‚ùå Erreur inattendue lors de l'appel √† Mistral : {e}")
            return None

    def run_scraper(self):
        """Ex√©cute le scraping et stocke les donn√©es en base."""
        html_content = self.fetch_html()
        if not html_content:
            print("‚ùå √âchec de la r√©cup√©ration du contenu HTML.")
            return

        job_list = self.extract_jobs_from_html(html_content)
        if not job_list:
            print("‚ùå Aucune offre trouv√©e.")
            return

        for job in job_list:
            job_url = job['url']
            print(f"üìå V√©rification de l'offre : {job_url}")

            # V√©rifier si l'offre existe d√©j√†
            if self.collection.find_one({"url": job_url}):
                print("‚ö†Ô∏è Offre d√©j√† existante dans la base de donn√©es. Ignor√©e.\n")
                continue  

            job_text = self.extract_full_text(job_url)
            if not job_text:
                print(f"‚ùå Impossible d'extraire le texte de l'offre : {job_url}")
                continue  

            resumeAI = self.summarize_with_mistral(job_text)
            if resumeAI is None:
                print(f"‚ùå L'API Mistral a √©chou√©, l'offre ne sera pas enregistr√©e : {job_url}\n")
                continue  

            category = self.categorize_job(job["title"])

            job_entry = {
                "title": job["title"],
                "company": job["company"],
                "location": job["location"],
                "url": job_url,
                "resume": resumeAI,
                "category": category,
                "created_at": datetime.utcnow()
            }

            try:
                result = self.collection.insert_one(job_entry)
                print(f"‚úÖ Offre enregistr√©e : {job['title']} (ID: {result.inserted_id}) | Cat√©gorie : {category}\n")
            except Exception as e:
                print(f"‚ùå Erreur lors de l'enregistrement dans MongoDB : {e}\n")

# D√©marrer un serveur Flask pour Render
@app.route("/")
def home():
    return jsonify({"message": "Job Scraper is running!"})

@app.route("/scrape")
def scrape():
    threading.Thread(target=scraper.run_scraper).start()
    return jsonify({"message": "Scraping started!"})

if __name__ == "__main__":
    scraper = JobScraper(
        url="https://www.mediacongo.net/emplois/",
        mongo_uri=offreBot.MONGO_URI,
        db_name="job_database",
        collection_name="jobs"
    )
    port = int(os.environ.get("PORT", 5000))
    app.run(host="0.0.0.0", port=port, debug=True)
